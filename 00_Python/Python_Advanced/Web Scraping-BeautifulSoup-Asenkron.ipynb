{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25298216",
   "metadata": {},
   "source": [
    "# Web Scraping-BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a306e48e",
   "metadata": {},
   "source": [
    "## Basic Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9eb19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4\n",
    "!pip install requests pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65576d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Hedef URL (canlı hava durumu verisi için örnek)\n",
    "url = \"https://example.com/weather\"  # Hedef siteyi buraya ekleyin\n",
    "\n",
    "# Web sayfasına HTTP isteği gönderme\n",
    "response = requests.get(url)\n",
    "\n",
    "# Sayfanın içeriğini BeautifulSoup ile parse etme\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Örneğin, hava durumu sayfasında bir div içerisindeki sıcaklık bilgisini çekmek için:\n",
    "# Bu, hedef web sayfasındaki HTML yapısına göre değişiklik gösterebilir\n",
    "temperature = soup.find('div', class_='temperature').get_text()\n",
    "\n",
    "# Sıcaklık bilgisini ekrana yazdırma\n",
    "print(f\"Güncel Sıcaklık: {temperature}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7fddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# 1. URL'den dosya indirmek\n",
    "url = \"https://example.com/data.csv\"  # Bu URL'yi indirilecek dosyanın gerçek URL'si ile değiştirin\n",
    "filename = \"downloaded_file.csv\"  # Kaydedilecek dosya adı\n",
    "\n",
    "# GET isteği ile dosyayı indirme\n",
    "response = requests.get(url)\n",
    "\n",
    "# Eğer isteğin durumu başarılı ise (200 OK)\n",
    "if response.status_code == 200:\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Dosya başarıyla indirildi ve '{filename}' adıyla kaydedildi.\")\n",
    "else:\n",
    "    print(\"Dosya indirilemedi. Hata kodu:\", response.status_code)\n",
    "\n",
    "# 2. İndirilen dosyayı Pandas ile okuma\n",
    "try:\n",
    "    df = pd.read_csv(filename)\n",
    "    print(\"Dosya başarıyla okundu.\")\n",
    "    print(df.head())  # İlk 5 satırı gösterme\n",
    "except Exception as e:\n",
    "    print(f\"Dosya okunurken bir hata oluştu: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2062c8",
   "metadata": {},
   "source": [
    " Açıklama:\n",
    "\n",
    "1. **Dosya İndirme**: \n",
    "   - `requests.get(url)` ile belirtilen URL'den dosya çekilir.\n",
    "   - Eğer **status_code** 200 (başarı) ise, dosya binary modda (`wb`) açılır ve içerik kaydedilir.\n",
    "\n",
    "2. **Pandas ile Dosya Okuma**: \n",
    "   - İndirilen dosya `pd.read_csv()` kullanılarak Pandas DataFrame'e yüklenir.\n",
    "   - `df.head()` ile ilk 5 satır ekrana yazdırılır.\n",
    "\n",
    " Örnek Senaryo:\n",
    "\n",
    "- Örneğin, bir hava durumu veya finansal veri sağlayıcısından CSV formatındaki verileri bu yöntemle indirip analiz edebilirsiniz.\n",
    "  \n",
    "Bu kod, **CSV** dosyası indirmek için tasarlanmıştır. Eğer dosya **Excel** ya da başka bir formatta olursa, uygun **Pandas** okuma yöntemini (`pd.read_excel()`, `pd.read_json()` gibi) kullanmanız gerekir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a404b4",
   "metadata": {},
   "source": [
    "## Python ile Asenkron Dosya İndirme ve Pandas ile Veri Okuma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aiohttp pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7249405",
   "metadata": {},
   "source": [
    "##### Yontem 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58efabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "\n",
    "# File URL and the name to save the file as\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/diabetes.csv\"\n",
    "filename = \"diabetes.csv\"\n",
    "\n",
    "# Asynchronous file download function\n",
    "async def download(url, filename):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            if response.status == 200:\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    f.write(await response.read())  # Download and save the file content\n",
    "                print(f\"File successfully downloaded and saved as '{filename}'.\")\n",
    "            else:\n",
    "                print(f\"Download failed. Error code: {response.status}\")\n",
    "\n",
    "# Start the asynchronous process\n",
    "async def main():\n",
    "    await download(url, filename)\n",
    "    # Read the downloaded file with pandas\n",
    "    df = pd.read_csv(filename)\n",
    "    print(\"First 5 rows:\")\n",
    "    print(df.head())\n",
    "\n",
    "# Run the asynchronous process\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6be86",
   "metadata": {},
   "source": [
    "Python'da modern kütüphaneleri kullanarak bu işlemi gerçekleştirmek için **`aiohttp`** kütüphanesi ile asenkron dosya indirme işlemi yapabiliriz. **`aiohttp`**, HTTP isteklerini asenkron olarak gerçekleştirmek için uygun bir kütüphanedir. Daha sonra, indirilen dosyayı **Pandas** ile okuyarak veri analizine geçebiliriz.\n",
    "\n",
    " Açıklama:\n",
    " \n",
    "1. **`aiohttp` ile Asenkron İndirme**:\n",
    "   - **`aiohttp.ClientSession()`** ile bir HTTP oturumu başlatıyoruz.\n",
    "   - **`session.get()`** ile URL'ye istek yapıyoruz ve yanıt 200 (başarılı) ise dosyayı kaydediyoruz.\n",
    "   \n",
    "2. **Pandas ile Dosya Okuma**:\n",
    "   - Dosya indirildikten sonra **Pandas** kullanarak CSV dosyasını okuyor ve ilk 5 satırını ekrana yazdırıyoruz.\n",
    "\n",
    "3. **Asenkron Çalıştırma**:\n",
    "   - **`asyncio.run(main())`** ifadesiyle asenkron işlemleri başlatıyoruz. Bu sayede dosya indirilip işlendikten sonra veriler üzerinde işlem yapabiliyoruz.\n",
    "\n",
    " Sonuç:\n",
    " \n",
    "Bu kod, Python'da **aiohttp** kullanarak bir URL'den dosya indirir ve ardından indirilen dosyayı **Pandas** ile okur. Asenkron yapısı sayesinde işlem hızlandırılır ve büyük veri dosyalarıyla çalışırken performans artırılır.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73222aa9",
   "metadata": {},
   "source": [
    "##### Yontem 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14473701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Dosya URL'si ve kaydedilecek dosya ismi\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/diabetes.csv\"\n",
    "filename = \"diabetes.csv\"\n",
    "\n",
    "# Dosya indirme işlemi\n",
    "response = requests.get(url)\n",
    "\n",
    "# Eğer istek başarılı olduysa, dosyayı kaydedelim\n",
    "if response.status_code == 200:\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"File successfully downloaded and saved as '{filename}'.\")\n",
    "else:\n",
    "    print(f\"Download failed. Error code: {response.status}\")\n",
    "\n",
    "# İndirilen dosyayı pandas ile okuma\n",
    "df = pd.read_csv(filename)\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659021f9",
   "metadata": {},
   "source": [
    "# Web Scraping Nedir?\n",
    "\n",
    "Web scraping, internet üzerindeki web sitelerinden veri toplamak için kullanılan bir tekniktir. Python, bu süreç için çeşitli kütüphaneler sunar ve yaygın olarak **requests** ve **BeautifulSoup** kütüphaneleri kullanılır. Web scraping, veri bilimi, pazar araştırmaları, fiyat karşılaştırmaları ve daha birçok alanda kullanılır.\n",
    "\n",
    "Aşağıda, web scraping işlemlerinin adım adım nasıl yapıldığı detaylıca açıklanmıştır.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Hedef Web Sitesinin Belirlenmesi ve Analizi**\n",
    "\n",
    "### A. **Amacın Belirlenmesi:**\n",
    "Öncelikle, hangi veriyi çekmek istediğinizi belirleyin. Örneğin, bir haber sitesinden başlıkları, bir e-ticaret sitesinden ürün fiyatlarını ya da hava durumu verilerini çekmek isteyebilirsiniz.\n",
    "\n",
    "### B. **HTML Yapısını İnceleme:**\n",
    "Web scraping işlemi yapılacak sitedeki verilerin nerede olduğunu anlamak için HTML yapısını analiz etmeniz gerekir. Bunun için tarayıcınızın \"İncele\" (Inspect) aracını kullanarak hedef verilerin bulunduğu HTML etiketlerini ve sınıflarını öğrenebilirsiniz.\n",
    "\n",
    "- Tarayıcınızda hedef sayfaya sağ tıklayıp \"İncele\" seçeneğini tıklayın.\n",
    "- Hedef verinin olduğu HTML etiketi ve sınıfı (class) burada görüntülenir. Bu bilgiler, BeautifulSoup ile veri çekmek için kullanılacaktır.\n",
    "\n",
    "### C. **Yasal Kontroller:**\n",
    "Bir web sitesinde scraping yapmadan önce, sitenin kullanım koşullarını (Terms of Service) kontrol edin ve `robots.txt` dosyasına bakarak hangi sayfaların taranmasına izin verildiğini öğrenin.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Gerekli Kütüphanelerin Kurulumu**\n",
    "\n",
    "Python’da web scraping işlemi için en yaygın kullanılan kütüphaneler **requests** ve **BeautifulSoup**'tur. Eğer dinamik sayfalardan veri çekecekseniz, **Selenium** gibi tarayıcı otomasyon kütüphanelerini de kullanabilirsiniz.\n",
    "\n",
    "### Kütüphanelerin Kurulumu:\n",
    "```bash\n",
    "pip install requests beautifulsoup4\n",
    "pip install selenium  # Dinamik içerik için kullanılır\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Web Sayfasına HTTP İsteği Gönderme**\n",
    "\n",
    "**requests** kütüphanesi ile hedef web sayfasına HTTP isteği gönderilir. Bu istek, sayfanın kaynak kodlarını (HTML) elde etmek içindir.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Sayfa başarılı şekilde yüklendi mi?\n",
    "if response.status_code == 200:\n",
    "    print(\"Sayfa başarıyla yüklendi!\")\n",
    "else:\n",
    "    print(\"Bir sorun oluştu, hata kodu:\", response.status_code)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Sayfa İçeriğini Parse Etme**\n",
    "\n",
    "Gelen HTML içeriğini, **BeautifulSoup** kütüphanesi kullanarak parse ederiz. BeautifulSoup, HTML veya XML belgelerini daha kolay işlenebilir bir forma dönüştürür.\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML içeriğini BeautifulSoup ile parse et\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "```\n",
    "\n",
    "### Parse Etme Yöntemleri:\n",
    "- **`html.parser`**: Python'un yerleşik parser'ıdır.\n",
    "- **`lxml`**: Daha hızlı ve eksik HTML belgeleriyle daha iyi çalışır.\n",
    "- **`html5lib`**: HTML5 uyumlu parser'dır.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Veri Bulma ve Çekme**\n",
    "\n",
    "Parse edilen HTML içinden veriyi çekmek için **BeautifulSoup** metotları kullanılır.\n",
    "\n",
    "- **`find()`**: İlk eşleşen HTML elementini bulur.\n",
    "- **`find_all()`**: Tüm eşleşen elementleri bulur ve bir liste döner.\n",
    "- **`get_text()`**: Bir elementin içindeki metni alır.\n",
    "\n",
    "### Örnek:\n",
    "```python\n",
    "# Hedef veriyi çekelim: Örneğin, kitap başlıklarını\n",
    "book_titles = soup.find_all('h3')\n",
    "\n",
    "# Çekilen başlıkları yazdıralım\n",
    "for title in book_titles:\n",
    "    print(title.get_text())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Verilerin Temizlenmesi ve Yapılandırılması**\n",
    "\n",
    "Veri çekildikten sonra, genellikle bazı temizleme işlemleri yapılması gerekir. Bu, boşlukların temizlenmesi, karakterlerin düzenlenmesi ya da sayısal değerlerin dönüştürülmesi olabilir. Ayrıca, çekilen verileri bir liste veya pandas **DataFrame** yapısına dönüştürebilirsiniz.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Başlıkları bir listeye alalım\n",
    "titles = [title.get_text().strip() for title in book_titles]\n",
    "\n",
    "# Listeyi pandas DataFrame'e dönüştürme\n",
    "df = pd.DataFrame({'Book Titles': titles})\n",
    "\n",
    "# İlk 5 veriyi göster\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Verilerin Depolanması**\n",
    "\n",
    "Çekilen ve yapılandırılan verileri bir dosyaya kaydedebilirsiniz. Verileri CSV, JSON veya bir veritabanına kaydetmek yaygın yöntemlerdendir.\n",
    "\n",
    "### Veriyi CSV Dosyasına Kaydetme:\n",
    "```python\n",
    "df.to_csv('book_titles.csv', index=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. **Sayfalar Arası Geçiş (Pagination)**\n",
    "\n",
    "Birçok web sitesinde veriler sayfalara bölünmüş olabilir. Bu durumda URL yapısını analiz ederek sayfalar arasında gezinebilir ve verileri toplu şekilde çekebilirsiniz.\n",
    "\n",
    "### Sayfalar Arası Geçiş Örneği:\n",
    "```python\n",
    "base_url = \"https://example.com/page-{}.html\"\n",
    "all_titles = []\n",
    "\n",
    "for page in range(1, 5):  # İlk 4 sayfa için veri çekme\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Veriyi çekme işlemleri\n",
    "    book_titles = soup.find_all('h3')\n",
    "    titles = [title.get_text().strip() for title in book_titles]\n",
    "    all_titles.extend(titles)\n",
    "\n",
    "# Tüm başlıkları yazdır\n",
    "print(all_titles)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. **Dinamik İçerik ve Selenium Kullanımı**\n",
    "\n",
    "Bazı web siteleri dinamik içerik yükler (örneğin, JavaScript ile). Bu durumda, **requests** ve **BeautifulSoup** yeterli olmayabilir. Bu tür durumlar için **Selenium** gibi araçlar kullanarak tarayıcı otomasyonu yapabilirsiniz.\n",
    "\n",
    "### Selenium Kullanımı Örneği:\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Chrome tarayıcısını başlat\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Hedef sayfaya git\n",
    "driver.get(\"https://example.com\")\n",
    "\n",
    "# Belirli bir sınıfa sahip öğeleri bul\n",
    "elements = driver.find_elements(By.CLASS_NAME, 'classname')\n",
    "\n",
    "# Öğelerin metnini yazdır\n",
    "for element in elements:\n",
    "    print(element.text)\n",
    "\n",
    "# Tarayıcıyı kapat\n",
    "driver.quit()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. **Yasal ve Etik Hususlar**\n",
    "\n",
    "- **robots.txt**: Web sitesinin `robots.txt` dosyasını inceleyerek hangi sayfaların taranmasına izin verildiğini öğrenin.\n",
    "- **Kullanım Koşulları**: Web sitesinin kullanım şartlarına uymak önemlidir. İzin verilmeyen sayfaları taramaktan kaçının.\n",
    "- **Sıklık Kontrolü**: Web scraping işlemi yaparken, çok fazla istekte bulunarak sunucuya aşırı yük bindirmemeye dikkat edin. İstekler arasında rastgele bekleme süreleri eklemek iyi bir yaklaşımdır.\n",
    "\n",
    "```python\n",
    "import time\n",
    "import random\n",
    "\n",
    "time.sleep(random.uniform(1, 3))  # 1 ile 3 saniye arasında rastgele bekle\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Toplu Kod Örneği: Kitap Başlıklarını Çekme\n",
    "## bir web sitesinden kitap başlıklarını çekmek için tüm aşamaları uygulayan bir kod örneği verilmiştir:\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Hedef URL\n",
    "base_url = \"http://books.toscrape.com/catalogue/page-{}.html\"\n",
    "all_titles = []\n",
    "\n",
    "# İlk 3 sayfa için veri çekme\n",
    "for page in range(1, 4):\n",
    "    url = base_url.format(page)\n",
    "    \n",
    "    # HTTP isteği gönder\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Sayfa içeriğini parse et\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Kitap başlıklarını çek\n",
    "    book_titles = soup.find_all('h3')\n",
    "    titles = [title.get_text().strip() for title in book_titles]\n",
    "    all_titles.extend(titles)\n",
    "    \n",
    "    # Sunucuya aşırı yüklenmemek için rastgele bekle\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "\n",
    "# Çekilen verileri DataFrame'e dönüştür\n",
    "df = pd.DataFrame({'Book Titles': all_titles})\n",
    "\n",
    "# Veriyi CSV dosyasına kaydet\n",
    "df.to_csv('book_titles.csv', index=False)\n",
    "\n",
    "print(\"Veri başarıyla çekildi ve kaydedildi.\")\n",
    "```\n",
    "\n",
    "### Sonuç:\n",
    "\n",
    "# - Hedef URL'deki sayfalardan kitap başlıkları çekilir.\n",
    "# - Her sayfa arasında rastgele bekleme süreleri eklenir.\n",
    "# - Çekilen veriler pandas DataFrame'e dönüştürülerek `book_titles.csv` dosyasına kaydedilir.\n",
    "\n",
    "# Bu süreç, web scraping'in temel aşamalarını kapsar ve veri çekme işlemlerini basitleştirir."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "278.338px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
